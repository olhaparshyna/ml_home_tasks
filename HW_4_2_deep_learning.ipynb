{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 1. Логістична регресія з нуля.**\n",
        "\n",
        "Будемо крок за кроком будувати модель лог регресії з нуля для передбачення, чи буде врожай більше за 80 яблук (задача подібна до лекційної, але на класифікацію).\n",
        "\n",
        "Давайте нагадаємо основні формули для логістичної регресії.\n",
        "\n",
        "### Функція гіпотези - обчислення передбачення у логістичній регресії:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\sigma(x W^T + b) = \\frac{1}{1 + e^{-(x W^T + b)}}\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ \\hat{y} $ — це ймовірність \"позитивного\" класу.\n",
        "- $ x $ — це вектор (або матриця для набору прикладів) вхідних даних.\n",
        "- $ W $ — це вектор (або матриця) вагових коефіцієнтів моделі.\n",
        "- $ b $ — це зміщення (bias).\n",
        "- $ \\sigma(z) $ — це сигмоїдна функція активації.\n",
        "\n",
        "### Як обчислюється сигмоїдна функція:\n",
        "\n",
        "Сигмоїдна функція $ \\sigma(z) $ має вигляд:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Ця функція перетворює будь-яке дійсне значення $ z $ в інтервал від 0 до 1, що дозволяє інтерпретувати вихід як ймовірність для логістичної регресії.\n",
        "\n",
        "### Формула функції втрат для логістичної регресії (бінарна крос-ентропія):\n",
        "\n",
        "Функція втрат крос-ентропії оцінює, наскільки добре модель передбачає класи, порівнюючи передбачені ймовірності $ \\hat{y} $ із справжніми мітками $ y $. Формула наступна:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = - \\left[ y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "Де:\n",
        "- $ y $ — це справжнє значення (мітка класу, 0 або 1).\n",
        "- $ \\hat{y} $ — це передбачене значення (ймовірність).\n",
        "\n"
      ],
      "metadata": {
        "id": "lbLHTNfSclli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.\n",
        "Тут вже наведений код для ініціювання набору даних в форматі numpy. Перетворіть `inputs`, `targets` на `torch` тензори. Виведіть результат на екран."
      ],
      "metadata": {
        "id": "GtOYB-RHfc_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3BNXSR-VdYKQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QLKZ77x4v_-v"
      },
      "outputs": [],
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "inputs, targets"
      ],
      "metadata": {
        "id": "KjoeaDrk6fO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4db8ef93-4328-41a9-a78f-ca591473cec0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.],\n",
              "         [102.,  43.,  37.],\n",
              "         [ 69.,  96.,  70.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Ініціюйте ваги `w`, `b` для моделі логістичної регресії потрібної форми зважаючи на розмірності даних випадковими значеннями з нормального розподілу. Лишаю тут код для фіксації `random_seed`."
      ],
      "metadata": {
        "id": "iKzbJKfOgGV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)"
      ],
      "metadata": {
        "id": "aXhKw6Tdj1-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10cb16e9-85bb-40d0-e565-e78b51df2093"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7c594c10a790>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(1, 3, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)"
      ],
      "metadata": {
        "id": "eApcB7eb6h9o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишіть функцію `model`, яка буде обчислювати функцію гіпотези в логістичній регресії і дозволяти робити передбачення на основі введеного рядка даних і коефіцієнтів в змінних `w`, `b`.\n",
        "\n",
        "  **Важливий момент**, що функція `model` робить обчислення на `torch.tensors`, тож для математичних обчислень використовуємо фукнціонал `torch`, наприклад:\n",
        "  - обчсилення $e^x$: `torch.exp(x)`\n",
        "  - обчсилення $log(x)$: `torch.log(x)`\n",
        "  - обчислення середнього значення вектору `x`: `torch.mean(x)`\n",
        "\n",
        "  Використайте функцію `model` для обчислення передбачень з поточними значеннями `w`, `b`.Виведіть результат обчислень на екран.\n",
        "\n",
        "  Проаналізуйте передбачення. Чи не викликають вони у вас підозр? І якщо викликають, то чим це може бути зумовлено?"
      ],
      "metadata": {
        "id": "nYGxNGTaf5s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(inputs, w, b):\n",
        "    logits = inputs @ w.t() + b\n",
        "\n",
        "    preds = 1 / (1 + torch.exp(-logits))\n",
        "\n",
        "    return preds\n",
        "\n",
        "preds = model(inputs, w, b)\n",
        "\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "pSz2j4Fh6jBv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd8d19e9-6756-49ba-f2ba-86dcce0d670b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Всі передбачення представлені класом 1. Ваги ініційовані випадковим чином, а також модель не пройшла навчання, тому і результати погані."
      ],
      "metadata": {
        "id": "fMxZ5rEpxeu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Напишіть функцію `binary_cross_entropy`, яка приймає на вхід передбачення моделі `predicted_probs` та справжні мітки в даних `true_labels` і обчислює значення втрат (loss)  за формулою бінарної крос-ентропії для кожного екземпляра та вертає середні втрати по всьому набору даних.\n",
        "  Використайте функцію `binary_cross_entropy` для обчислення втрат для поточних передбачень моделі."
      ],
      "metadata": {
        "id": "O2AGM0Mb2yHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_cross_entropy(predicted_probs, true_labels):\n",
        "    epsilon = 1e-7\n",
        "\n",
        "    loss = -(true_labels * torch.log(predicted_probs + epsilon) +\n",
        "             (1 - true_labels) * torch.log(1 - predicted_probs + epsilon))\n",
        "\n",
        "    return loss.mean()\n",
        "\n",
        "\n",
        "loss = binary_cross_entropy(preds, targets)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "1bWlovvx6kZS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "672f819c-d26f-4dc8-adf6-42f820e63cb4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.4472, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Зробіть зворотнє поширення помилки і виведіть градієнти за параметрами `w`, `b`. Проаналізуйте їх значення. Як гадаєте, чому вони саме такі?"
      ],
      "metadata": {
        "id": "ZFKpQxdHi1__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "print(w)\n",
        "print(w.grad)\n",
        "\n",
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "id": "YAbXUNSJ6mCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd31cb3f-8e3b-48f0-aa5c-8cddf288e31c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.6614, 0.2669, 0.0617]], requires_grad=True)\n",
            "tensor([[1.0201e-22, 9.3628e-23, 6.0090e-23]])\n",
            "tensor([0.6213], requires_grad=True)\n",
            "tensor([1.3974e-24])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ми маємо дуже маленькі градієнти. Маємо неоптимальну модель, бо ваги були ініційовано не оптимально."
      ],
      "metadata": {
        "id": "NDg9G_ql0s-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Що сталось?**\n",
        "\n",
        "В цій задачі, коли ми ініціювали значення випадковими значеннями з нормального розподілу, насправді ці значення не були дуже гарними стартовими значеннями і привели до того, що градієнти стали дуже малими або навіть рівними нулю (це призводить до того, що градієнти \"зникають\"), і відповідно при оновленні ваг у нас не буде нічого змінюватись. Це називається `gradient vanishing`. Це відбувається через **насичення сигмоїдної функції активації.**\n",
        "\n",
        "У нашій задачі ми використовуємо сигмоїдну функцію активації, яка має такий вигляд:\n",
        "\n",
        "   $$\n",
        "   \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "   $$\n",
        "\n",
        "\n",
        "Коли значення $z$ дуже велике або дуже мале, сигмоїдна функція починає \"насичуватись\". Це означає, що для великих позитивних $z$ сигмоїда наближається до 1, а для великих негативних — до 0. В цих діапазонах градієнти починають стрімко зменшуватись і наближаються до нуля (бо градієнт - це похідна, похідна на проміжку функції, де вона паралельна осі ОХ, дорівнює 0), що робить оновлення ваг неможливим.\n",
        "\n",
        "![](https://editor.analyticsvidhya.com/uploads/27889vaegp.png)\n",
        "\n",
        "У логістичній регресії $ z = x \\cdot w + b $. Якщо ваги $w, b$ - великі, значення $z$ також буде великим, і сигмоїда перейде в насичену область, де градієнти дуже малі.\n",
        "\n",
        "Саме це сталося в нашій задачі, де великі випадкові значення ваг викликали насичення сигмоїдної функції. Це в свою чергу призводить до того, що під час зворотного поширення помилки (backpropagation) модель оновлює ваги дуже повільно або зовсім не оновлює. Це називається проблемою **зникнення градієнтів** (gradient vanishing problem).\n",
        "\n",
        "**Що ж робити?**\n",
        "Ініціювати ваги маленькими значеннями навколо нуля. Наприклад ми можемо просто в існуючій ініціалізації ваги розділити на 1000. Можна також використати інший спосіб ініціалізації вагів - інформація про це [тут](https://www.geeksforgeeks.org/initialize-weights-in-pytorch/).\n",
        "\n",
        "Як це робити - показую нижче. **Виконайте код та знову обчисліть передбачення, лосс і виведіть градієнти.**\n",
        "\n",
        "А я пишу пояснення, чому просто не зробити\n",
        "\n",
        "```\n",
        "w = torch.randn(1, 3, requires_grad=True)/1000\n",
        "b = torch.randn(1, requires_grad=True)/1000\n",
        "```\n",
        "\n",
        "Нам потрібно, аби тензори вагів були листовими (leaf tensors).\n",
        "\n",
        "1. **Що таке листовий тензор**\n",
        "Листовий тензор — це тензор, який був створений користувачем безпосередньо і з якого починається обчислювальний граф. Якщо такий тензор має `requires_grad=True`, PyTorch буде відслідковувати всі операції, виконані над ним, щоб правильно обчислювати градієнти під час навчання.\n",
        "\n",
        "2. **Чому ми використовуємо `w.data` замість звичайних операцій**\n",
        "Якщо ми просто виконали б операції, такі як `(w - 0.5) / 100`, ми б отримали **новий тензор**, який вже не був би листовим тензором, оскільки ці операції створюють **новий** тензор, а не модифікують існуючий.\n",
        "\n",
        "  Проте, щоб залишити наші тензори ваги `w` та зміщення `b` листовими і продовжити можливість відстеження градієнтів під час тренування, ми використовуємо атрибут `.data`. Цей атрибут дозволяє **виконувати операції in-place (прямо на існуючому тензорі)** без зміни самого об'єкта тензора. Отже, тензор залишається листовим, і PyTorch може коректно обчислювати його градієнти.\n",
        "\n",
        "3. **Чому важливо залишити тензор листовим**\n",
        "Якщо тензор більше не є листовим (наприклад, через проведення операцій, що створюють нові тензори), ви не зможете отримати градієнти за допомогою `w.grad` чи `b.grad` після виклику `loss.backward()`. Це може призвести до втрати можливості оновлення параметрів під час тренування моделі. В нашому випадку ми хочемо, щоб тензори `w` та `b` накопичували градієнти, тому вони повинні залишатись листовими.\n",
        "\n",
        "**Висновок:**\n",
        "Ми використовуємо `.data`, щоб виконати операції зміни значень на ваги і зміщення **in-place**, залишаючи їх листовими тензорами, які можуть накопичувати градієнти під час навчання. Це дозволяє коректно працювати механізму зворотного поширення помилки (backpropagation) і оновлювати ваги моделі."
      ],
      "metadata": {
        "id": "nDN1t1RujQsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Виконайте код та знову обчисліть передбачення, лосс і знайдіть градієнти та виведіть всі ці тензори на екран."
      ],
      "metadata": {
        "id": "rOPSQyttpVjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(1)\n",
        "w = torch.randn(1, 3, requires_grad=True)  # Листовий тензор\n",
        "b = torch.randn(1, requires_grad=True)     # Листовий тензор\n",
        "\n",
        "# in-place операції\n",
        "w.data = w.data / 1000\n",
        "b.data = b.data / 1000"
      ],
      "metadata": {
        "id": "-EBOJ3tsnRaD"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs, w, b)\n",
        "print('preds:', preds)\n",
        "\n",
        "loss = binary_cross_entropy(preds, targets)\n",
        "print('loss:', loss)\n",
        "\n",
        "loss.backward()\n",
        "print('w:', w)\n",
        "print('w.grad:', w.grad)\n",
        "\n",
        "print('b:', b)\n",
        "print('b.grad:', b.grad)"
      ],
      "metadata": {
        "id": "-JwXiSpX6orh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c660ce6-46ec-4e31-f3d6-29feff1da4c8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "preds: tensor([[0.5174],\n",
            "        [0.5220],\n",
            "        [0.5244],\n",
            "        [0.5204],\n",
            "        [0.5190]], grad_fn=<MulBackward0>)\n",
            "loss: tensor(0.6829, grad_fn=<MeanBackward0>)\n",
            "w: tensor([[6.6135e-04, 2.6692e-04, 6.1677e-05]], requires_grad=True)\n",
            "w.grad: tensor([[ -5.4417, -18.9853, -10.0682]])\n",
            "b: tensor([0.0006], requires_grad=True)\n",
            "b.grad: tensor([-0.0794])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Напишіть алгоритм градієнтного спуску, який буде навчати модель з використанням написаних раніше функцій і виконуючи оновлення ваг. Алгоритм має включати наступні кроки:\n",
        "\n",
        "  1. Генерація прогнозів\n",
        "  2. Обчислення втрат\n",
        "  3. Обчислення градієнтів (gradients) loss-фукнції відносно ваг і зсувів\n",
        "  4. Налаштування ваг шляхом віднімання невеликої величини, пропорційної градієнту (`learning_rate` домножений на градієнт)\n",
        "  5. Скидання градієнтів на нуль\n",
        "\n",
        "Виконайте градієнтний спуск протягом 1000 епох, обчисліть фінальні передбачення і проаналізуйте, чи вони точні?"
      ],
      "metadata": {
        "id": "RCdi44IT334o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1000):\n",
        "    preds = model(inputs, w, b)\n",
        "    loss = binary_cross_entropy(preds, targets)\n",
        "    loss.backward()\n",
        "    with torch.no_grad():\n",
        "        w -= w.grad * 1e-5\n",
        "        b -= b.grad * 1e-5\n",
        "        w.grad.zero_()\n",
        "        b.grad.zero_()\n",
        "\n",
        "preds = model(inputs, w, b)\n",
        "loss = binary_cross_entropy(preds, targets)\n",
        "print('loss:', loss)\n",
        "print('preds:', preds)\n"
      ],
      "metadata": {
        "id": "mObHPyE06qsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e08f8e4-2a78-4c9f-91d6-815f548d158b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: tensor(0.3357, grad_fn=<MeanBackward0>)\n",
            "preds: tensor([[0.5777],\n",
            "        [0.6685],\n",
            "        [0.9113],\n",
            "        [0.1616],\n",
            "        [0.8653]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Передбачення стали кращими, тепер представлен і клас 0, проте все одно є помилка для першого екземпляру, можливо можна змінити трешхолд на 0.6."
      ],
      "metadata": {
        "id": "SKLgASlU9Tr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Секція 2. Створення лог регресії з використанням функціоналу `torch.nn`.**\n",
        "\n",
        "Давайте повторно реалізуємо ту ж модель, використовуючи деякі вбудовані функції та класи з PyTorch.\n",
        "\n",
        "Даних у нас буде побільше - тож, визначаємо нові масиви."
      ],
      "metadata": {
        "id": "fuRhlyF9qAia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вхідні дані (temp, rainfall, humidity)\n",
        "inputs = np.array([[73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70],\n",
        "                   [73, 67, 43],\n",
        "                   [91, 88, 64],\n",
        "                   [87, 134, 58],\n",
        "                   [102, 43, 37],\n",
        "                   [69, 96, 70]], dtype='float32')\n",
        "\n",
        "# Таргети (apples > 80)\n",
        "targets = np.array([[0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1],\n",
        "                    [1],\n",
        "                    [0],\n",
        "                    [1]], dtype='float32')"
      ],
      "metadata": {
        "id": "IX8Bhm74rV4M"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Завантажте вхідні дані та мітки в PyTorch тензори та з них створіть датасет, який поєднує вхідні дані з мітками, використовуючи клас `TensorDataset`. Виведіть перші 3 елементи в датасеті.\n",
        "\n"
      ],
      "metadata": {
        "id": "7X2dV30KtAPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "inputs = torch.from_numpy(inputs)\n",
        "targets = torch.from_numpy(targets)\n",
        "\n",
        "train_ds = TensorDataset(inputs, targets)\n",
        "train_ds[0:3]"
      ],
      "metadata": {
        "id": "chrvMfBs6vjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dd2e8b1-bf4f-40f8-a336-586f94f37a3e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 73.,  67.,  43.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 87., 134.,  58.]]),\n",
              " tensor([[0.],\n",
              "         [1.],\n",
              "         [1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Визначте data loader з класом **DataLoader** для підготовленого датасету `train_ds`, встановіть розмір батчу на 5 та увімкніть перемішування даних для ефективного навчання моделі. Виведіть перший елемент в дата лоадері."
      ],
      "metadata": {
        "id": "4nMFaa8suOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
        "next(iter(train_dl))"
      ],
      "metadata": {
        "id": "ZCsRo5Mx6wEI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0d8854-6330-4c1e-eced-439a23b97437"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 87., 134.,  58.],\n",
              "         [ 91.,  88.,  64.],\n",
              "         [ 73.,  67.,  43.],\n",
              "         [ 73.,  67.,  43.],\n",
              "         [ 69.,  96.,  70.]]),\n",
              " tensor([[1.],\n",
              "         [1.],\n",
              "         [0.],\n",
              "         [0.],\n",
              "         [1.]])]"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Створіть клас `LogReg` для логістичної регресії, наслідуючи модуль `torch.nn.Module` за прикладом в лекції (в частині про FeedForward мережі).\n",
        "\n",
        "  У нас модель складається з лінійної комбінації вхідних значень і застосування фукнції сигмоїда. Тож, нейромережа буде складатись з лінійного шару `nn.Linear` і використання активації `nn.Sigmid`. У створеному класі мають бути реалізовані методи `__init__` з ініціалізацією шарів і метод `forward` для виконання прямого проходу моделі через лінійний шар і функцію активації.\n",
        "\n",
        "  Створіть екземпляр класу `LogReg` в змінній `model`."
      ],
      "metadata": {
        "id": "ymcQOo_hum6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LogReg(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LogReg, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear(x)\n",
        "        preds = self.sigmoid(logits)\n",
        "        return preds\n",
        "\n",
        "input_dim = 3\n",
        "model = LogReg(input_dim)\n",
        "\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "EyAwhTBW6xxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f26ca23-625d-48f9-ea89-f785f6ede49d"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg(\n",
            "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Задайте оптимізатор `Stockastic Gradient Descent` в змінній `opt` для навчання моделі логістичної регресії. А також визначіть в змінній `loss` функцію втрат `binary_cross_entropy` з модуля `torch.nn.functional` для обчислення втрат моделі. Обчисліть втрати для поточних передбачень і міток, а потім виведіть їх. Зробіть висновок, чи моделі вдалось навчитись?"
      ],
      "metadata": {
        "id": "RflV7xeVyoJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "opt = torch.optim.SGD(model.parameters(), 1e-5)\n",
        "\n",
        "preds = model(inputs)\n",
        "\n",
        "loss_fn = F.binary_cross_entropy\n",
        "loss = loss_fn(preds, targets)\n",
        "\n",
        "print(f'loss: {loss.item():.4f}')\n",
        "opt.zero_grad()\n"
      ],
      "metadata": {
        "id": "3QCATPU_6yfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25adddf0-780d-42b3-e983-2e11e226c242"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 2.0292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds.int()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34Oh__HyDIy7",
        "outputId": "94839df0-0cc8-44cc-c5db-5627ecad818e"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель не навчилась, має дуже низькі значення передбачень, тому маємо скрізь клас 0."
      ],
      "metadata": {
        "id": "1_oxQK-7Eby5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Візьміть з лекції функцію для тренування моделі з відстеженням значень втрат і навчіть щойно визначену модель на 1000 епохах. Виведіть після цього графік зміни loss, фінальні передбачення і значення таргетів."
      ],
      "metadata": {
        "id": "ch-WrYnKzMzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Модифікована функцію fit для відстеження втрат\n",
        "def fit_return_loss(num_epochs, model, loss_fn, opt, train_dl):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Ініціалізуємо акумулятор для втрат\n",
        "        total_loss = 0\n",
        "\n",
        "        for xb, yb in train_dl:\n",
        "            # Генеруємо передбачення\n",
        "            pred = model(xb)\n",
        "\n",
        "            # Обчислюємо втрати\n",
        "            loss = loss_fn(pred, yb)\n",
        "\n",
        "            # Виконуємо градієнтний спуск\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "\n",
        "            # Накопичуємо втрати\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Обчислюємо середні втрати для епохи\n",
        "        avg_loss = total_loss / len(train_dl)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        # Виводимо підсумок епохи\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "          print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "    return losses\n",
        "\n",
        "loss = fit_return_loss(1000, model, loss_fn, opt, train_dl)"
      ],
      "metadata": {
        "id": "rPUEoOIQielJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42253666-fcb4-4377-9fbe-3698fdfc8322"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 1.9943\n",
            "Epoch [20/1000], Loss: 1.9290\n",
            "Epoch [30/1000], Loss: 1.8762\n",
            "Epoch [40/1000], Loss: 1.8306\n",
            "Epoch [50/1000], Loss: 1.7709\n",
            "Epoch [60/1000], Loss: 1.7220\n",
            "Epoch [70/1000], Loss: 1.6865\n",
            "Epoch [80/1000], Loss: 1.6325\n",
            "Epoch [90/1000], Loss: 1.5814\n",
            "Epoch [100/1000], Loss: 1.5370\n",
            "Epoch [110/1000], Loss: 1.4983\n",
            "Epoch [120/1000], Loss: 1.4665\n",
            "Epoch [130/1000], Loss: 1.4115\n",
            "Epoch [140/1000], Loss: 1.3726\n",
            "Epoch [150/1000], Loss: 1.3409\n",
            "Epoch [160/1000], Loss: 1.3019\n",
            "Epoch [170/1000], Loss: 1.2697\n",
            "Epoch [180/1000], Loss: 1.2577\n",
            "Epoch [190/1000], Loss: 1.2126\n",
            "Epoch [200/1000], Loss: 1.1816\n",
            "Epoch [210/1000], Loss: 1.1554\n",
            "Epoch [220/1000], Loss: 1.1379\n",
            "Epoch [230/1000], Loss: 1.1190\n",
            "Epoch [240/1000], Loss: 1.0899\n",
            "Epoch [250/1000], Loss: 1.0661\n",
            "Epoch [260/1000], Loss: 1.0478\n",
            "Epoch [270/1000], Loss: 1.0290\n",
            "Epoch [280/1000], Loss: 1.0112\n",
            "Epoch [290/1000], Loss: 0.9951\n",
            "Epoch [300/1000], Loss: 0.9884\n",
            "Epoch [310/1000], Loss: 0.9679\n",
            "Epoch [320/1000], Loss: 0.9522\n",
            "Epoch [330/1000], Loss: 0.9426\n",
            "Epoch [340/1000], Loss: 0.9302\n",
            "Epoch [350/1000], Loss: 0.9178\n",
            "Epoch [360/1000], Loss: 0.9084\n",
            "Epoch [370/1000], Loss: 0.8975\n",
            "Epoch [380/1000], Loss: 0.8897\n",
            "Epoch [390/1000], Loss: 0.8815\n",
            "Epoch [400/1000], Loss: 0.8713\n",
            "Epoch [410/1000], Loss: 0.8628\n",
            "Epoch [420/1000], Loss: 0.8583\n",
            "Epoch [430/1000], Loss: 0.8523\n",
            "Epoch [440/1000], Loss: 0.8418\n",
            "Epoch [450/1000], Loss: 0.8380\n",
            "Epoch [460/1000], Loss: 0.8309\n",
            "Epoch [470/1000], Loss: 0.8228\n",
            "Epoch [480/1000], Loss: 0.8193\n",
            "Epoch [490/1000], Loss: 0.8113\n",
            "Epoch [500/1000], Loss: 0.8055\n",
            "Epoch [510/1000], Loss: 0.8055\n",
            "Epoch [520/1000], Loss: 0.7956\n",
            "Epoch [530/1000], Loss: 0.7933\n",
            "Epoch [540/1000], Loss: 0.7866\n",
            "Epoch [550/1000], Loss: 0.7833\n",
            "Epoch [560/1000], Loss: 0.7836\n",
            "Epoch [570/1000], Loss: 0.7762\n",
            "Epoch [580/1000], Loss: 0.7688\n",
            "Epoch [590/1000], Loss: 0.7674\n",
            "Epoch [600/1000], Loss: 0.7661\n",
            "Epoch [610/1000], Loss: 0.7578\n",
            "Epoch [620/1000], Loss: 0.7595\n",
            "Epoch [630/1000], Loss: 0.7503\n",
            "Epoch [640/1000], Loss: 0.7480\n",
            "Epoch [650/1000], Loss: 0.7440\n",
            "Epoch [660/1000], Loss: 0.7431\n",
            "Epoch [670/1000], Loss: 0.7372\n",
            "Epoch [680/1000], Loss: 0.7341\n",
            "Epoch [690/1000], Loss: 0.7336\n",
            "Epoch [700/1000], Loss: 0.7294\n",
            "Epoch [710/1000], Loss: 0.7262\n",
            "Epoch [720/1000], Loss: 0.7229\n",
            "Epoch [730/1000], Loss: 0.7200\n",
            "Epoch [740/1000], Loss: 0.7182\n",
            "Epoch [750/1000], Loss: 0.7144\n",
            "Epoch [760/1000], Loss: 0.7131\n",
            "Epoch [770/1000], Loss: 0.7103\n",
            "Epoch [780/1000], Loss: 0.7172\n",
            "Epoch [790/1000], Loss: 0.7078\n",
            "Epoch [800/1000], Loss: 0.7120\n",
            "Epoch [810/1000], Loss: 0.7018\n",
            "Epoch [820/1000], Loss: 0.7001\n",
            "Epoch [830/1000], Loss: 0.6964\n",
            "Epoch [840/1000], Loss: 0.6960\n",
            "Epoch [850/1000], Loss: 0.6926\n",
            "Epoch [860/1000], Loss: 0.6887\n",
            "Epoch [870/1000], Loss: 0.6869\n",
            "Epoch [880/1000], Loss: 0.6845\n",
            "Epoch [890/1000], Loss: 0.6868\n",
            "Epoch [900/1000], Loss: 0.6811\n",
            "Epoch [910/1000], Loss: 0.6785\n",
            "Epoch [920/1000], Loss: 0.6771\n",
            "Epoch [930/1000], Loss: 0.6770\n",
            "Epoch [940/1000], Loss: 0.6733\n",
            "Epoch [950/1000], Loss: 0.6774\n",
            "Epoch [960/1000], Loss: 0.6723\n",
            "Epoch [970/1000], Loss: 0.6668\n",
            "Epoch [980/1000], Loss: 0.6679\n",
            "Epoch [990/1000], Loss: 0.6658\n",
            "Epoch [1000/1000], Loss: 0.6623\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "oIk1uhVdE8rR",
        "outputId": "87257fdd-6606-489d-f555-1ce57de175e9"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASIpJREFUeJzt3XlYVPXiBvD3DAPDIgz7Koj7hiIuIC6lhamZZbbrdWu7lppmq5m2Gt66mbc0/XW7ZotL6VVvmWmKWyouqCi4gIYIIoMCwrAOMHN+f1BHJxABhzmzvJ/nmeeZc+acmXeO6byd7SuIoiiCiIiIyEYo5A5AREREZEosN0RERGRTWG6IiIjIprDcEBERkU1huSEiIiKbwnJDRERENoXlhoiIiGyKUu4A5mYwGHD58mW4u7tDEAS54xAREVEjiKKIkpISBAcHQ6FoeN+M3ZWby5cvIzQ0VO4YRERE1AzZ2dlo3bp1g8vYXblxd3cHULtxPDw8ZE5DREREjaHVahEaGir9jjfE7srNn4eiPDw8WG6IiIisTGNOKeEJxURERGRTWG6IiIjIprDcEBERkU1huSEiIiKbwnJDRERENoXlhoiIiGwKyw0RERHZFJYbIiIisiksN0RERGRTWG6IiIjIprDcEBERkU1huSEiIiKbwnJjQsXl1UjTlMgdg4iIyK6x3JjIubwSRL77Kx5efgCiKModh4iIyG6x3JhIqLcrAKCksgbz/3dK5jRERET2i+XGRJwdHaTn3x68KGMSIiIi+8Zy00JKKqvljkBERGSXWG5M6NMnoqTnmuJKGZMQERHZL5YbE7o/MhidA9wBAMM+2YsV+y7InIiIiMj+yFpu4uPj0a9fP7i7u8Pf3x9jxoxBWlraLddbt24dunTpAmdnZ/To0QNbtmwxQ9rG+fPEYgB4d/NpGZMQERHZJ1nLzZ49ezBt2jQcPHgQ27dvR3V1Ne655x6UlZXddJ0DBw7giSeewFNPPYXjx49jzJgxGDNmDFJTU82Y/OaiwjzljkBERGTXBNGCbspy9epV+Pv7Y8+ePbjjjjvqXeaxxx5DWVkZNm/eLM3r378/evXqheXLl9dZXqfTQafTSdNarRahoaEoLi6Gh4eHyb9DUmYhHl6eKE1nLhxl8s8gIiKyN1qtFmq1ulG/3xZ1zk1xcTEAwNvb+6bLJCYmIi4uzmje8OHDkZiYWO/y8fHxUKvV0iM0NNR0gevRO8zLaDq/VHeTJYmIiKglWEy5MRgMmDVrFgYOHIiIiIibLqfRaBAQEGA0LyAgABqNpt7l58yZg+LiYumRnZ1t0tx/pVAI2DH7+l6nvu/vQFWNoUU/k4iIiK5Tyh3gT9OmTUNqair27dtn0vdVqVRQqVQmfc9b6eDvbjSdW1yBNj5uZs1ARERkryxiz8306dOxefNm7Nq1C61bt25w2cDAQOTl5RnNy8vLQ2BgYEtGvC285w0REZH5yFpuRFHE9OnTsXHjRuzcuRNt27a95TqxsbFISEgwmrd9+3bExsa2VMxmeSL6+rk9Gi3LDRERkbnIWm6mTZuG7777DqtXr4a7uzs0Gg00Gg0qKiqkZSZOnIg5c+ZI0zNnzsTWrVvx8ccf4+zZs3j77beRlJSE6dOny/EVburt+7tLzy9dq2hgSSIiIjIlWcvNsmXLUFxcjCFDhiAoKEh6fP/999IyWVlZyM3NlaYHDBiA1atX44svvkBkZCTWr1+PTZs2NXgSshxUSgfMv68bAGD76bxbLE1ERESmYlH3uTGHplwnf7vytJWI+SABggCcemc4XJ0s5vxtIiIiq2K197mxNQEezvBzV0EUgTRNidxxiIiI7ALLTQvrGlTbLk9kF8kbhIiIyE6w3LSwwR18AQDbTvG8GyIiInNguWlhIyJq77+TmFGApMxCmdMQERHZPpabFhbq7YqIkNpDU+//fEbmNERERLaP5cYM3hpde8+bE5eKeLdiIiKiFsZyYwb9wr3Rs7Uaogh8tvOc3HGIiIhsGsuNmTw9uB2A2r03v6TkIplXTxEREbUI3lXOTHqEqAEAqTlaPLfqGAAgc+EoOSMRERHZJO65MZMwb1e4Oxt3STu7OTQREZFZsNyYiYNCwLShHYzmVVYbZEpDRERku1huzOiRPq2NprWV1TIlISIisl0sN2bk00qFqDBPabqE5YaIiMjkWG7MbNXTMdLzLSkaGZMQERHZJpYbM3N1un5S8aLt6TAYeFIxERGRKbHcyOytH0/JHYGIiMimsNzI4OV7OknPvz14UcYkREREtoflRgbT7+oodwQiIiKbxXJDRERENoXlRiafPRElPV99KEvGJERERLaF5UYm9/UMkp6/sTFFxiRERES2heVGJoIgyB2BiIjIJrHcyGjzjEHS8+V7fpcxCRERke1guZFRRIhaGm9q4S9nUVmtlzkRERGR9WO5kdnCh3pKz/u9v0PGJERERLaB5UZmDgoBkaGeAIASXQ30HI6BiIjotrDcWIAlN1wWrtFWypiEiIjI+rHcWIBQb1eE+7gCAJKziiCK3HtDRETUXCw3FqJrkAcAYNrqY/jH1jSZ0xAREVkvlhsL8WjfUOk5LwsnIiJqPpYbC9HBv5XcEYiIiGwCy42FCFI7yx2BiIjIJrDcWAilg/EfRbXeIFMSIiIi68ZyY0HWPttfer5k53kZkxAREVkvlhsL0r+dD5wda/9ITudqZU5DRERknVhuLMz/TegLAMgqKJc5CRERkXViubEwbbxrb+Z3sbAMBg7FQERE1GQsNxYmxMsFAFBZbUC7N7bg/JUSmRMRERFZF1nLzd69ezF69GgEBwdDEARs2rTpluusWrUKkZGRcHV1RVBQEJ588kkUFBS0fFgzcXRQYGhnP2l619mrMqYhIiKyPrKWm7KyMkRGRmLp0qWNWn7//v2YOHEinnrqKZw6dQrr1q3D4cOH8cwzz7RwUvN6496u0vM8DqRJRETUJEo5P3zkyJEYOXJko5dPTExEeHg4XnjhBQBA27Zt8fe//x3/+Mc/brqOTqeDTqeTprVay78KqWOAO94c1RXv/3wGX+67gKcGt0WQ2kXuWERERFbBqs65iY2NRXZ2NrZs2QJRFJGXl4f169fj3nvvvek68fHxUKvV0iM0NPSmy1oSDxdH6fmGYzkyJiEiIrIuVlVuBg4ciFWrVuGxxx6Dk5MTAgMDoVarGzysNWfOHBQXF0uP7OxsMyZuvohgtfRcV8O7FRMRETWWVZWb06dPY+bMmZg/fz6OHj2KrVu3IjMzE1OnTr3pOiqVCh4eHkYPa9At2AODO/oCAC4V8p43REREjWVV5SY+Ph4DBw7EK6+8gp49e2L48OH4/PPPsWLFCuTm5sodz+Qe61d7CG3D8Rycv1IqcxoiIiLrYFXlpry8HAqFcWQHBwcAgCja3g3v+rTxkp6/+H2yfEGIiIisiKzlprS0FMnJyUhOTgYAXLhwAcnJycjKygJQe77MxIkTpeVHjx6NDRs2YNmyZcjIyMD+/fvxwgsvIDo6GsHBwXJ8hRZ14xVSKTnFMiYhIiKyHrKWm6SkJERFRSEqKgoAMHv2bERFRWH+/PkAgNzcXKnoAMDkyZOxaNEiLFmyBBEREXjkkUfQuXNnbNiwQZb85rDx+QHS86nfHkVOUYWMaYiIiCyfINri8ZwGaLVaqNVqFBcXW8XJxTV6A3q8/SsqqvUAgGlD2+OV4V1kTkVERGReTfn9tqpzbuyR0kGBN+69XmYKy6plTENERGT5WG6swITYcLw5qnZIBm0Fyw0REVFDWG6shL+HMwDg55RcnMvjSOFEREQ3w3JjJTxvGI7hu4MXZUxCRERk2VhurER7/1bSc2dHBxmTEBERWTaWGysR4umCEd0DAQBXSnS3WJqIiMh+sdxYkRERteXmeNY1m7wjMxERkSmw3FiR7sG11/VnFpTjwO8FMqchIiKyTCw3VqRjgDvu7VG79+bnFNsbKJSIiMgUWG6szNio1gCAtYezkF1YLnMaIiIiy8NyY2Xu7uqPyNZqGERgd9oVueMQERFZHJYbKyMIAoZ1CwAALN+TgWq9QeZEREREloXlxgpNiA2Hi6MDcooqkKbh3YqJiIhuxHJjhdQujujdxhMAkJJTzMvCiYiIbsByY6V6hXoCAOZsSEG/BTuQVcCTi4mIiACWG6s1rFug9Dy/tAr//i1DxjRERESWg+XGSkW2VqO9n5s07aAQZExDRERkOVhurJQgCIj746opAHB0YLkhIiICWG6s2t/vaC89//dvF6Cr0cuYhoiIyDKw3FgxbzcnvHFvF2m685tbUcP73hARkZ1jubFyfdp4G02fuqyVKQkREZFlYLmxcn3aeOGJ6FBpOrOgTMY0RERE8mO5sQEfPNgDod4uAIDfr7LcEBGRfWO5sQGCIGDygLYAgGMXr/G8GyIismssNzaiTxsvAMC+8/kY9+UhmdMQERHJh+XGRnQP9pCeH75QCL2B400REZF9YrmxEY4Oxn+UBaU6mZIQERHJi+XGhvzzkUjp+ZUSlhsiIrJPLDc25OE+rRERUnt4Kk9bKXMaIiIiebDc2Jggde0l4TPXJvPQFBER2SWWGxszLjoMAFCqq8G7m0/LnIaIiMj8WG5szNAu/njvge4AgF9SNCgqr5I5ERERkXmx3NigCbHh6BrkgSq9AZtP5sodh4iIyKxYbmzUQ71DAAD/PXZJ5iRERETmxXJjox7oFQKFABzPKsIVXjlFRER2hOXGRvm5qxDu6wYAOJ2rlTkNERGR+bDc2LAOfq0AAJO/OgIDh2MgIiI7wXJjw/4cTBMAzmi494aIiOyDrOVm7969GD16NIKDgyEIAjZt2nTLdXQ6HebOnYs2bdpApVIhPDwcK1asaPmwVujJQW3h4awEAOw7ly9zGiIiIvOQtdyUlZUhMjISS5cubfQ6jz76KBISEvCf//wHaWlpWLNmDTp37tyCKa2Xo4MCs+I6AQB2pV2ROQ0REZF5KOX88JEjR2LkyJGNXn7r1q3Ys2cPMjIy4O3tDQAIDw9vcB2dTged7vowBFqtfR2euaOTLwDgYEYhhn+yF5umDYSLk4PMqYiIiFqOVZ1z8+OPP6Jv37748MMPERISgk6dOuHll19GRUXFTdeJj4+HWq2WHqGhoWZMLL/2f5xUDABpeSV47b8nZUxDRETU8qyq3GRkZGDfvn1ITU3Fxo0bsXjxYqxfvx7PP//8TdeZM2cOiouLpUd2drYZE8tPEATMHtZJmv7xxGUZ0xAREbU8qyo3BoMBgiBg1apViI6Oxr333otFixbh66+/vuneG5VKBQ8PD6OHvXnh7o5yRyAiIjIbqyo3QUFBCAkJgVqtluZ17doVoiji0iUOM9CQV0dcP+n6pxOXUVmtlzENERFRy7GqcjNw4EBcvnwZpaWl0rz09HQoFAq0bt1axmSW77k728NJWfvHPWPNcSz85azMiYiIiFqGrOWmtLQUycnJSE5OBgBcuHABycnJyMrKAlB7vszEiROl5ceNGwcfHx9MmTIFp0+fxt69e/HKK6/gySefhIuLixxfwWoIgoBOAddPLl55IFO+MERERC1I1nKTlJSEqKgoREVFAQBmz56NqKgozJ8/HwCQm5srFR0AaNWqFbZv346ioiL07dsX48ePx+jRo/Hpp5/Kkt/axLT1kTsCERFRixNEUbSrQYe0Wi3UajWKi4vt7uTiE9lFeGDpfmk6/f2R0qEqIiIiS9aU32/+stmRyFBPLHgwQprOLCiTMQ0REVHLYLmxM+Nj2qBXqCcA4H/JOfKGISIiagEsN3ZocMfaIRm+P3IJdnZUkoiI7ADLjR16bkh7AEB+qQ5XS3S3WJqIiMi6sNzYIVcnJdr7uQEA3tl8WuY0REREpsVyY6dG9QwGABz8vQCiKCK/lHtwiIjINrDc2KlpQ2vvWFxQVoUXv09G3/d34CcOqklERDaA5cZOqZQOuKOjHwBgU3JtqXll/Qk5IxEREZkEy40dm/mX0cIVgiBTEiIiItNhubFjPVqrERXmKU07sNwQEZENYLmxc0M6+UvP2W2IiMgWsNzYuScHhcPdWQkAqKjWo7CsSuZEREREt4flxs65Ozvi5Fv3oHuwB6r1InacyZM7EhER0W1huSEIgoDeYV4AgAv5HEyTiIisG8sNAQDa+LgCALIKymVOQkREdHtYbggAEOZdW27O5Go5mCYREVk1lhsCAPQN94ZKqUBGfhn+9p9D0NXo5Y5ERETULCw3BADwdnPCm6O6AgD2ny/AvnP5MiciIiJqHpYbkkyIDccjfVoDAA78XiBzGiIiouZhuSEjd3SqHW8qgZeEExGRlWK5ISOx7X0AAJkF5Vi0PV3mNERERE3HckNGfFup8NKwTgCATxPO4ddTGpkTERERNQ3LDdUx4+6OGB0ZDAB49tujSM0pljkRERFR47HcUL2iQj2l51/+liFfECIioiZiuaF6ebs5Sc83JV/Gst2/y5iGiIio8VhuqF5DO/sbTf9j61mZkhARETUNyw3VS+3qiH893kvuGERERE3GckM31bO1p9E0x5wiIiJrwHJDN9XW1w339QySpkt0NTKmISIiahyWG2rQZ09ESScXr9yfKW8YIiKiRmC5oQYJgoDnh7QHAPyQlA2DgYemiIjIsrHc0C39rX8btFIpcelaBd768RT0LDhERGTBWG7olpwdHXBvj0AAwLcHL2LT8RyZExEREd0cyw01ypioEOl5wlmOGE5ERJaL5YYaJbadD/zcVQCArMJyXhZOREQWi+WGGkUQBHz4UE8AQGqOFh9sOSNzIiIiovqx3FCj9WvrDQ9nJQBgS4pG5jRERET1Y7mhRmulUuK3V+8CAOQUVSC3uELmRERERHXJWm727t2L0aNHIzg4GIIgYNOmTY1ed//+/VAqlejVq1eL5aO61K6OaOfrBgB496fTMqchIiKqS9ZyU1ZWhsjISCxdurRJ6xUVFWHixIm4++67WygZNeSdB7oDALad0uCKtlLmNERERMaUcn74yJEjMXLkyCavN3XqVIwbNw4ODg633Nuj0+mg0+mkaa1W2+TPI2ODO/qhR4gaKTnFeGfzaSwd11vuSERERBKrO+fmq6++QkZGBt56661GLR8fHw+1Wi09QkNDWzihfbiriz8AYEtKLorLq2VOQ0REdJ1VlZtz587h9ddfx3fffQelsnE7nebMmYPi4mLpkZ2d3cIp7cPUO2vHmxJF4I1NKTKnISIiuq5Z5SY7OxuXLl2Spg8fPoxZs2bhiy++MFmwv9Lr9Rg3bhzeeecddOrUqdHrqVQqeHh4GD3o9rk4OeDhPq0BAD+fzMXRi4UyJyIiIqrVrHIzbtw47Nq1CwCg0WgwbNgwHD58GHPnzsW7775r0oB/KikpQVJSEqZPnw6lUgmlUol3330XJ06cgFKpxM6dO1vkc+nmXhneGYJQ+/yhZYk8PEVERBahWeUmNTUV0dHRAIAffvgBEREROHDgAFatWoWVK1eaMp/Ew8MDKSkpSE5Olh5Tp05F586dkZycjJiYmBb5XLq5AA9nfH7DycS706/ImIaIiKhWs66Wqq6uhkpVO87Qjh07cP/99wMAunTpgtzc3Ea/T2lpKc6fPy9NX7hwAcnJyfD29kZYWBjmzJmDnJwcfPPNN1AoFIiIiDBa39/fH87OznXmk/kM6xYgPf/9SqmMSYiIiGo1a89N9+7dsXz5cvz222/Yvn07RowYAQC4fPkyfHx8Gv0+SUlJiIqKQlRUFABg9uzZiIqKwvz58wEAubm5yMrKak5EMhOlgwJvjuoKAPh053lcLdHdYg0iIqKWJYjNGN559+7dePDBB6HVajFp0iSsWLECAPDGG2/g7Nmz2LBhg8mDmopWq4VarUZxcTFPLjaRc3klGLN0P8qq9ACA7S/egY4B7jKnIiIiW9KU3+9mlRug9uolrVYLLy8vaV5mZiZcXV3h7+/fnLc0C5ablnEoowCPfXEQADC2dwgWPdpL3kBERGRTmvL73azDUhUVFdDpdFKxuXjxIhYvXoy0tDSLLjbUcqLbekvPf79Sio9/TcORTF4eTkRE5tescvPAAw/gm2++AVA7zlNMTAw+/vhjjBkzBsuWLTNpQLIOgiBg/dRYAMCJS8X4bOd5PLI8UeZURERkj5pVbo4dO4bBgwcDANavX4+AgABcvHgR33zzDT799FOTBiTr0dHfHUqFIHcMIiKyc80qN+Xl5XB3rz1h9Ndff8XYsWOhUCjQv39/XLx40aQByXqoXR3x/NAORvOq9QaZ0hARkb1qVrnp0KEDNm3ahOzsbGzbtg333HMPAODKlSs8SdfOPTO4rdH0FV4aTkREZtascjN//ny8/PLLCA8PR3R0NGJja8+1+PXXX6V71pB9cnd2xN/vbCdNXyoslzENERHZo2ZfCq7RaJCbm4vIyEgoFLUd6fDhw/Dw8ECXLl1MGtKUeCl4yxNFEc+vOoZfUjUAgLPvjYCzo4PMqYiIyJq1+KXgABAYGIioqChcvnxZGiE8OjraoosNmYcgCBgREShN379kH0p1NTImIiIie9KscmMwGPDuu+9CrVajTZs2aNOmDTw9PfHee+/BYOAJpATcHxmMUT2CAADpeaX4JjFT3kBERGQ3mlVu5s6diyVLlmDhwoU4fvw4jh8/jg8++ACfffYZ5s2bZ+qMZIUEQcA/Hu4pTe87ly9jGiIisifNGhX866+/xpdffimNBg4APXv2REhICJ5//nksWLDAZAHJerVSKbHgwQjM3ZiKkkoeliIiIvNo1p6bwsLCes+t6dKlCwoLect9ui46vHZYht+vlmLeplSMWLwXZTz/hoiIWlCzyk1kZCSWLFlSZ/6SJUvQs2fPetYge9XerxWC1M4or9Lj24MXcVZTggc/3y93LCIismHNOiz14YcfYtSoUdixY4d0j5vExERkZ2djy5YtJg1I1k2hEDAhtg0+3JomzUvPK8WZXC26BvFSfCIiMr1m7bm58847kZ6ejgcffBBFRUUoKirC2LFjcerUKXz77bemzkhW7tnB7erMO3elVIYkRERkD5p9E7/6nDhxAr1794ZerzfVW5ocb+Inj91pVzD5qyPS9HND2uO1EbwnEhERNY5ZbuJH1BRDOvuje/D1/xizOCwDERG1EJYbMpuVU6Lh5eoIgGNOERFRy2G5IbPxc1fhu6djAAAnLhXj96s874aIiEyvSVdLjR07tsHXi4qKbicL2YFwHzfp+fLdv+OjRyJlTENERLaoSeVGrVbf8vWJEyfeViCybW433LV4a6oG9/YIgrayGg/0CpE7GhER2QiTXi1lDXi1lPwMBhH3LN6L8zdcDp7w0p1o79dKxlRERGTJeLUUWTSFQsDUO9sbzcsq4AnGRERkGiw3JIsxvYLRL9xLmtZoK2VMQ0REtoTlhmShdFDg26diMKiDLwBgzoYUHL3IQVeJiOj2sdyQbJwdHdCnzfW9Ny+sScbwT/bii72/y5iKiIisHcsNyWp8/zDEdQ0AAOQUVSAtrwQfbDkrcyoiIrJmLDckK393Z3w5qS86B7gbzbezi/iIiMiEWG7IIgzq6Gs0nV9aJVMSIiKydiw3ZBGeGdzOaJoDaxIRUXOx3JBFCFQ7Y3RksDT90LIDqKzWy5iIiIisFcsNWYyPH4lEVJinNH3qsla+MEREZLVYbshiOCkVeGlYZ2l637l8aCurZUxERETWiOWGLMqgjr64o5MfAOCTHekY+/kBmRMREZG1Ybkhi/PsDScXn79SioJSnYxpiIjI2rDckMUZ1NEXr464fngqlefeEBFRE7DckEV67s726N/OGwDw0g/JuMKBNYmIqJFkLTd79+7F6NGjERwcDEEQsGnTpgaX37BhA4YNGwY/Pz94eHggNjYW27ZtM09YMitBEPCvx6PQztcN+aVVWH/sktyRiIjISshabsrKyhAZGYmlS5c2avm9e/di2LBh2LJlC44ePYqhQ4di9OjROH78eAsnJTkEeDjjkb6hAIAPt6Zh59k8mRMREZE1EEQLGcRHEARs3LgRY8aMadJ63bt3x2OPPYb58+fX+7pOp4NOd/2EVK1Wi9DQUBQXF8PDw+N2IpMZXC6qwPgvD+FCfpk07/tn+yOmnY+MqYiIyNy0Wi3UanWjfr+t+pwbg8GAkpISeHt733SZ+Ph4qNVq6REaGmrGhHS7gj1dsP3FOzD4hrGnXlp3QsZERERk6ay63Pzzn/9EaWkpHn300ZsuM2fOHBQXF0uP7OxsMyYkU1A6KDB5QLg0XVltkC8MERFZPKXcAZpr9erVeOedd/C///0P/v7+N11OpVJBpVKZMRm1hH5tr++dK9PVoKJKDxcnBxkTERGRpbLKPTdr167F008/jR9++AFxcXFyxyEz8HB2ROKcu9BKpURFtR4bj+fIHYmIiCyU1ZWbNWvWYMqUKVizZg1GjRoldxwyoyC1C2be3REA8MbGFLz4fTIs5Hx4IiKyILKWm9LSUiQnJyM5ORkAcOHCBSQnJyMrKwtA7fkyEydOlJZfvXo1Jk6ciI8//hgxMTHQaDTQaDQoLi6WIz7JYFxMGDxdHQEAG4/n4FhWkbyBiIjI4shabpKSkhAVFYWoqCgAwOzZsxEVFSVd1p2bmysVHQD44osvUFNTg2nTpiEoKEh6zJw5U5b8ZH5uKiU+fKinNH0iu0i+MEREZJEs5j435tKU6+TJcj37TRJ+PV17U7/593XDlIHhEARB5lRERNRS7OY+N2S/3n8wAp0CWgEA3t18GhNXHJY5ERERWQqWG7JK/u7O2DbrDmn6t3P52JqqkTERERFZCpYbslqCIGDh2B7S9NTvjsqYhoiILAXLDVm1x6PDMKJ7oDRdWFYlYxoiIrIELDdk9ZaO7w1/99q7UPd+bztSc3hrACIie8ZyQ1bPQSFg4UPXD0/d99k+7D+fL2MiIiKSE8sN2YS7ugRgzTP9penxXx7Cgd9ZcIiI7BHLDdmM/u28EeLpIk1PW3VMxjRERCQXlhuyGYIgYPcrQ9A7zBMAcK28GptPXpY3FBERmR3LDdkURwcFNjw/EBEhtXevfOen0ygo1cmcioiIzInlhmzSkid6w8XRAVdLdJj63VHU6A1yRyIiIjNhuSGbFO7rhh+nD4S7SokjmdfQ7a1tGPzhTlwsKJM7GhERtTCWG7JZHQPcsfCPEcSragzILqxA/JazMqciIqKWxnJDNm1UzyCMjLh+B+OtpzTYdfaKjImIiKilsdyQzZs9rJPR9JSVR2RKQkRE5sByQzavY4A7dsy+02ieKIoypSEiopbGckN2oYN/KzzSp7U0veFYjoxpiIioJbHckN346JFI3NnJDwDw5qZUXOMI4kRENonlhuzKl5P6okugOyqq9Yh6bzuW7jovdyQiIjIxlhuyK44OCrxwd0dp+qNtaXh1/QkZExERkamx3JDdubdHEF4f2UWa/iHpEn49pZExERERmRLLDdmlqXe2x7LxvaXpZ789inVJ2TAYeBUVEZG1Y7khuzWyRxB+e3WoNP3K+pNYcyRLxkRERGQKLDdk10K9XTGwg480vSUll4NsEhFZOZYbsnv/mdQP74+JAADsP1+ADnN/QWpOscypiIiouVhuyO45Ozrgb/3bYEyvYGnes98kYemu8ygq571wiIisDcsN0R/eGt0dvcM8AQCXiyvx0bY09Hp3O6p5mIqIyKqw3BD9wcvNCRueH4jlf+ttNH/OhhRUVutlSkVERE3FckP0F8O7B6JvGy9pev3RS3jw8wMsOEREVoLlhugvBEHAqmdijOadydUi4cwVmRIREVFTsNwQ1UOldEDmwlFYMi4Kff7YizNt9TFM+eowb/RHRGThWG6IGnBfz2DMv6+bNL0r7Sp2nMlDIUcUJyKyWCw3RLcQGeqJB268TPzbo3hy5REZExERUUNYbogaIX5sD6Pp5OwiXLpWLlMaIiJqCMsNUSO4OilxZG4c1C6O0rxB/9iFrakcTZyIyNKw3BA1kp+7Csnzh+HDh3pK86Z+dxSPLk/EubwSGZMREdGNWG6ImkAQBDzStzVevqeTNO9wZiG+O3hRxlRERHQjlhuiJhIEAdOGdsCdnfykeV8nXkRJZbWMqYiI6E+ylpu9e/di9OjRCA4OhiAI2LRp0y3X2b17N3r37g2VSoUOHTpg5cqVLZ6T6K8EQcDXT0Zj50t3SvN6vP0r8rSV0FZWo7yqRsZ0RET2TdZyU1ZWhsjISCxdurRRy1+4cAGjRo3C0KFDkZycjFmzZuHpp5/Gtm3bWjgpUf3a+bXCK8M7S9MxHySg59u/YuznB5CZX4ZSHUsOEZG5CaIoWsTtVgVBwMaNGzFmzJibLvPaa6/h559/RmpqqjTv8ccfR1FREbZu3VrvOjqdDjqdTprWarUIDQ1FcXExPDw8TJaf7NvqQ1l4Y2NKnflxXQPw5aS+MiQiIrItWq0WarW6Ub/fVnXOTWJiIuLi4ozmDR8+HImJiTddJz4+Hmq1WnqEhoa2dEyyQ+NiwrD3laF15u84kydDGiIi+2ZV5Uaj0SAgIMBoXkBAALRaLSoqKupdZ86cOSguLpYe2dnZ5ohKdijMxxVbZw2uMz+nqP7/NomIqGUo5Q7Q0lQqFVQqldwxyE50CfTAifn34KmvjyDp4jUAwMCFOwEAd3Tyw9dT+kEQBDkjEhHZPKvacxMYGIi8POPd/Hl5efDw8ICLi4tMqYiMqV0dsf65ARhzw3hUALA3/Sp2nLkiUyoiIvthVeUmNjYWCQkJRvO2b9+O2NhYmRIR3dwnj/XCrpeHQHHDjppnvknC/vP50Bss4jx+IiKbJGu5KS0tRXJyMpKTkwHUXuqdnJyMrKwsALXny0ycOFFafurUqcjIyMCrr76Ks2fP4vPPP8cPP/yAF198UY74RA0SBAFtfd3w9ZPRiG3nI80f/+UhfLI9HaIowkIuViQisimyXgq+e/duDB1a9wqTSZMmYeXKlZg8eTIyMzOxe/duo3VefPFFnD59Gq1bt8a8efMwefLkRn9mUy4lIzKVGr0B9yzei4yrZUbzB3XwxZeT+sLZ0UGmZERE1qEpv98Wc58bc2G5IbkYDCL+/t1RbD9tfN7Y26O7oXcbL1TrRfRp4yVTOiIiy8Zy0wCWG5Lb1RIdPk04h2//GGxTEIA//xYenHM3AtXOMqYjIrJMNnsTPyJb4OeuwntjInDy7XsQrHbGjf97se2URr5gREQ2guWGSCYezo5Y99wARIV5SvMO/J6P1JxiFJdzhHEioubiYSkiC/C/5BzMXJtsNO/QG3cjwIOHqIiIAB6WIrI6o3oEGV0uDgBTvzsKALxcnIioibjnhshCGAwiTudqcd9n+6R5gR7O0NXoUWMQ8fOMwQjzcZUxIRGRfLjnhsgKKRQCIkLUODz3boR41g4notFW4lp5NUoqa/DtwUx5AxIRWQmWGyIL4+/ujP2v34U1z/SXSg4A7E67Ck1xpYzJiIisAw9LEVmwymo9/rPvAj7alibNC1Y7419PRKFfuLeMyYiIzIs38WsAyw1Zo6yCcjz9zRGk55UazX9leGdMG9pBplRERObDctMAlhuyVmW6Gny28zyW7/ndaL6rkwPa+LhhRPdAPDekPZyUPNpMRLaHJxQT2SA3lRKvj+yCxDl3IfyGq6bKq/Q4k6vFJzvS8X1StowJiYgsA8sNkZUJUrtg50tDcOqd4RjQ3vjeOPM2peL8ldKbrElEZB9YboiskEIhwE2lxMop0Xi8XyjcnZXSa3GL9uDdn07j41/TUFReJWNKIiJ58JwbIhux6Nc0fHPwIopuGJeqa5AHvpzUFyGeLsgqKIe7sxJebk4ypiQiah6eUNwAlhuyZQaDiH9sO4v/25NhNH9cTBj+e/QSOga0wuYZg2VKR0TUfE35/VY2+CoRWRWFQsCckV0xrGsACsqq8Pdva8enWn0oCwCQmqNFdmE5Qr05jAMR2S7uuSGyYUcvFuKhZYl15r8yvDMKSqsQ4uWCpwa1lSEZEVHTcM8NEQEA+rTxxpl3R+CznefwQ1I28ktrTzC+8Y7HAR4qjOoRBEEQ5IpJRGRS3HNDZCeqagxYsvMcPt15vs5rfu4qfD0lGt2C+XeCiCwTb+JHRHU4KRWYfU9npL8/EhueH4DI1mrptaslOtz76W/49uBFGAzX/3+nsKwKm09ehq5GL0dkIqJm4Z4bIjsliiJScooxc20yLuSXGb02onsgPhsXhb99eQiHLhRyDCsikh0vBW8Ayw1RXV8fyMRbP54ymtfBv5V0t+NQbxf89updckQjIgLAw1JE1ESTBoQjc+EobJt1B6LDvQHAaBiH7MIK/HtvBg9PEZFV4J4bIjIiiiJ2p1/F+qRL+Dklt95l7uzkh5VT+vEKKyIyGx6WagDLDVHjFZdXY3FCOr7an1nnNR83J3w1pR82Hs+Bn7sKzw/hOTlE1HJYbhrAckPUdLoaPX5J0eDLfRlIzdHWu8ybo7ri6cHtzJyMiOwFy00DWG6Ibk+13oBfT+XhjY0p0FZW48Z/QToFtEJWYTkqqw24r2cQXhzWCe39WskXlohsBstNA1huiEyjokqPaoMBhzMK8fQ3SfUu08bHFbtfHgJBEGAwiFAoeI4OETUPy00DWG6ITK+8qgYz1yZj++m8Oq+5OysxsL0vtp/Jw9JxvTEiIlCGhERk7VhuGsByQ9RyisursffcVXy28xzS80rrXSZ+bA/oDSKUCgGPR4eZOSERWSuWmwaw3BCZR05RBVYfuoilu36/6TKvjeiC54a0B1B7Lk9qTjF6hXryEnMiqoPlpgEsN0TmJYoizmpK0EqlxJubUrEn/arR64M7+uK+nkFYdSgLJy8V45XhnTG8eyAC1c5wcXQAADjwXB0iu8dy0wCWGyL51OgNOHGpGICIh5YlNrjskM5+yC4sh4NCwE8zBkGldDBPSCKySCw3DWC5IbIcJy8V4ZPt6diVdrXB5e7q4o+X7+mMbsH8O0tkr1huGsByQ2R5RFHEoQuF2JN+Fct23/wcnRHdA/HMHW0RFerFy8qJ7AzLTQNYbogsW43egN1pV5GcXQS9KN607Dx7RztMGRiOH45cwqVr5ZhxV0eE+biaOS0RmYvVlZulS5fio48+gkajQWRkJD777DNER0ffdPnFixdj2bJlyMrKgq+vLx5++GHEx8fD2dn5lp/FckNkXX49pcHu9Ku4cLUMiRkFDS676ukYlOlqkF9ahcEdfeGmUsLbzclMSYmoJTXl91tppkw39f3332P27NlYvnw5YmJisHjxYgwfPhxpaWnw9/evs/zq1avx+uuvY8WKFRgwYADS09MxefJkCIKARYsWyfANiKgl3dM9EPd0v37jv/8evYQv913Amdy6Y1yN//KQ0XR7Pzd8Pr4Pwrxd4eLEE5KJ7IXse25iYmLQr18/LFmyBABgMBgQGhqKGTNm4PXXX6+z/PTp03HmzBkkJCRI81566SUcOnQI+/btu+Xncc8NkW1IyizE5eJKHMoowKpDWY1aZ/KAcLw1ups0HET8L2cQ7OmCKQPbtnBaIrpdVrPnpqqqCkePHsWcOXOkeQqFAnFxcUhMrP8y0QEDBuC7777D4cOHER0djYyMDGzZsgUTJkyod3mdTgedTidNa7X1j2hMRNalb7g3AOD+yGDMH90Npy9r8e3Bi9h/Ph/aihpUVOvrrLPyQCZWHsiEu7MSDgoBReXVAICxUa1RUa2HbysnKB0UZv0eRGR6spab/Px86PV6BAQEGM0PCAjA2bNn611n3LhxyM/Px6BBgyCKImpqajB16lS88cYb9S4fHx+Pd955x+TZichyqJQOiArzQlSYlzQv5VIxnvkmCRptZZ3lSyprjKb7fbADVTUG/P3OdojrGgAfNye042jmRFZL1sNSly9fRkhICA4cOIDY2Fhp/quvvoo9e/bg0KFDddbZvXs3Hn/8cbz//vuIiYnB+fPnMXPmTDzzzDOYN29eneXr23MTGhrKw1JEduRaWRU2p+Ri9aGses/VqY+3mxOGdw/ABw/24HAQRBbAag5L+fr6wsHBAXl5xiMJ5+XlITCw/pGD582bhwkTJuDpp58GAPTo0QNlZWV49tlnMXfuXCgUxruUVSoVVCpVy3wBIrIKXm5OmNC/DSb0b4NrZVUo1dXgTK4WvUI9sSUlFz+dzMXRi9eM1iksq8Kaw9no2doT93QLgE8rFf579BJOXdZiZlxHqF0cZfo2RHQrspYbJycn9OnTBwkJCRgzZgyA2hOKExISMH369HrXKS8vr1NgHBxqr4KwgKvaicjCebk5wcvNCaHetffEmTywLSYPbIutqRp8sOUMisqroL3hsNWcDSmYsyEFCgEw/PFPTFF5FToFumNLSi7+M6kf/Nz5P1BElkT2S8Fnz56NSZMmoW/fvoiOjsbixYtRVlaGKVOmAAAmTpyIkJAQxMfHAwBGjx6NRYsWISoqSjosNW/ePIwePVoqOURETTUiIhAjIgIhiiKOZRVhd9oV/N/eDFTVGABcLzYAsOF4jvS834IdaO/nht+vluG9MRGY0L+NuaMT0V/Ifik4ACxZskS6iV+vXr3w6aefIiYmBgAwZMgQhIeHY+XKlQCAmpoaLFiwAN9++y1ycnLg5+eH0aNHY8GCBfD09LzlZ/FScCJqLL1BRHZhOc5qSvBzSi68XR2xO/0qLhaU33LdN0d1RWW1Hn3DvdG/nQ++ScyEIAgsP0TNZHV3KDYnlhsiuh2iKCItrwQXC8rx/Kpj0Bua9k/oisl9MbSzP09SJmoilpsGsNwQkSlVVutRYxDx9YFMFJRWYcX+CwCAILUzcovrXob+p5i23vBwcUTnAHdsOHYJ3UPU+OSxXnB1dOCgoET1YLlpAMsNEbUkg0FEld4AZ0cH/HTiMmasOQ4AcHF0qPfGgn/V3s8Nm2cMhpNSgZ9OXMbADr7wc1dBFEXu7SG7xnLTAJYbIpLLr6c02HEmDz+fzEVZ1c2LTmRrNU5cKgYAuKuUGNzJFwczCvGPh3oirisPaZF9YrlpAMsNEVkCURRx6VoFWnu5QG8Q8dp/U/DfY5cate7gjr44nlWEUl0NIkI80CPEE12D3DExNhzvbz6Nw5mF+PapGN6Lh2wKy00DWG6IyJKdvqzFgd/zcfJSMX48cblJ6w7u6IvfzuUDqL1aa0hnf3Tw5zASZBtYbhrAckNE1kQURVRWG1BtMOCrfZnILCjDxhvus3Mr04a2R3mVHh393XE6txh/v6O9dANDImvCctMAlhsisnaiKCI9rxTHs65hUEdfBHg4Y8HPZ7DyQGaj1n9tRBd0DmyFPK0O93QLQJ5Wh65B7jyXhyway00DWG6IyFZV6w1QCAJOXS7GiewirDt6CSf/ODH5VgZ18EXXIHcUllWjra8rdDUGXCuvQnmVHq8M74wgtUsLpydqGMtNA1huiMielOlqpPvtbD55GYt3nEPnAHfkFlcYjaF1Kx38WyF+bA90DfLAnrSrcFAAIyKCWio2UR0sNw1guSEiqpVxtRT7z+dD6aBA4u8FTT6BuZ2fG6pqDOgc4I5uwR6YPCAczo4OcFPJPmwh2SCWmwaw3BAR3VyN3oCzmhJcyC/DL6m52HX2KjoFtJLuu9NY0eHe+GBsBP699wK+T8qGIADhPm745yM90aeNd4PrGgwiKqr1LElkhOWmASw3RERNp6vRI6ugHJkF5UjJKUa13oBtqRpAADKuljXpvdr5uiEy1BNxXQOQp63EnvSrmD2sEyJDPQEAnyWcwyc70rHmmf6IaedjtO6xrGu4dK0C90cGm+qrkZVguWkAyw0RkWlVVuuRW1yJiwVlKK6oxtZUDX49nQe9QWz0sBMA4KRUINTLBb/fUJaiwjzx7OB2GNzJDzNWH8OutKsAgA3PD0DvMC8AQGFZFT7beQ5TBrRFmA8vc7dVLDcNYLkhImp5oiiirEoPN6facrPp+GUs2XkO+WVVqKox3Pb739HJD7OHdUKXQHcMXLgTBWVV8G2lQtKbcXWWzdNWIjO/DOXVekSHe/Nwl5ViuWkAyw0RkfzKdDW4kF8G3R9F5/sjWUjJ0eJMrrZJ7+Pj5oSCsippelZcRzwRHYYyXQ3KdHq4Oysx5J+7pddHRwbjo4d74opWx708VoblpgEsN0RElqtGb4CuxoAyXQ3WHb2ES9cqoDcY8ENS7bhb8+/rhi/2ZkCjrWz2Z9zZyQ97z13FN09Go7CsCkO7+MPDmeNwWTqWmwaw3BARWbdqvQHVegO2n87DxYJyXCwoR1ZhGUI8XbA7/SqKyqub/d7BamfMuLsj0jQlOKvR4oFeIXigVzBW7LuAksoaPD24HfzcVSb8NtRYLDcNYLkhIrJdNXoDkrOLcOqyFtV6A66W6rDjdJ7RScq3w91ZiRfu6oi4bgFo6+sGACgo1cHT1QkKARzCogWx3DSA5YaIyD7V6A3ILChHSWU1DmYUQu3iiBqDAd8kXoSDIMBJqYBKqUDSxWuNej9vNycYRFHaU+SgEODurMTwboF4bkh7OCkV0BtEo4FKq2oMyCmqkIoRNR7LTQNYboiI6FZyiipQWFqFbxIzkVeiQ982XvB0dcT8/51q8nuFeNaOy+Xt5oSUnNqbIb4Y1wkz7uqAMxotfNxUWHskC1dKdLivRxBi2/twD1A9WG4awHJDRETNdbVEByelAjtO52FTcg7KdDXI0+rg76HC8awik32Ok4MCXm6OcFIq8GifUAR5uuCh3iEor9JDpVRA6aCos84n29MhApg9rBPO5GrR2ssF7jZ0ojTLTQNYboiIqCWIogiNthKiWHsn5YoqPUQROJ2rxenLWhzOLEREiAdKKmtwsaC8zvpKhQCDKMJwk1/l2HY+SMwoQHS4N54a3BZrDmeha5AHHunTGgpBkC55fyI6DGsOZ+HeHoH4fHwfAMDG45ewJ+0qFj7UE86ODi21CVoUy00DWG6IiEhuxRXV2HE6DxEhaqTnlSCmrTf8PZxx9GIh3v/5jMn2AkW39cbIiEC889NpALX3AWrr64bM/HL8kpqLD8b2kO70bOlYbhrAckNERJauvKoGrk5KHMooQGFZFRQKAWsOZ2H3H8NPeDgroa2sMclnhXi6oJVKiXZ+bhjS2Q9dAj3wyY50PN4vFCMigkzyGabActMAlhsiIrJmf/5s1xhEpOeVwNVJicpqPbak5CIp8xoqqvXo384Hqw9dNFkBCvBQ4e6uATh/pRS6GgMmxbbBwYwC/JB0Cf+e2BfDugWY5HMawnLTAJYbIiKyF3qDiMtFFWilUmL76Tx0CXKHl6sTdqVdgYNCwIZjOTh1uRhuTkqjYSyaykmpQFWNAb3DPPFI31AM6uCL1l4uJr3qi+WmASw3RERE1+kNIhwUAkRRRImuBgfO5+NyUSWOZV3D6VwtSitr0EqlRIiXC3KKKpDRiBsiOikVSH17OJyUda/qaq6m/H5zaFQiIiI75qCo3bsiCAI8nB2l82yeRNt6ly8o1SEtrwTt/VohwMMZmuJKfH8kG2sOZ0GjrYS3mxM6+rcyabFpKu65ISIiIpMyGEQoFKa9EWFTfr/lq1VERERkk0xdbJr8+bJ+OhEREZGJsdwQERGRTWG5ISIiIpvCckNEREQ2heWGiIiIbArLDREREdkUlhsiIiKyKSw3REREZFMsotwsXboU4eHhcHZ2RkxMDA4fPtzg8kVFRZg2bRqCgoKgUqnQqVMnbNmyxUxpiYiIyJLJPrbU999/j9mzZ2P58uWIiYnB4sWLMXz4cKSlpcHf37/O8lVVVRg2bBj8/f2xfv16hISE4OLFi/D09DR/eCIiIrI4so8tFRMTg379+mHJkiUAAIPBgNDQUMyYMQOvv/56neWXL1+Ojz76CGfPnoWjo2OTP49jSxEREVkfqxlbqqqqCkePHkVcXJw0T6FQIC4uDomJifWu8+OPPyI2NhbTpk1DQEAAIiIi8MEHH0Cv19e7vE6ng1arNXoQERGR7ZK13OTn50Ov1yMgIMBofkBAADQaTb3rZGRkYP369dDr9diyZQvmzZuHjz/+GO+//369y8fHx0OtVkuP0NBQk38PIiIishyyn3PTVAaDAf7+/vjiiy/g4OCAPn36ICcnBx999BHeeuutOsvPmTMHs2fPlqaLi4sRFhbGPThERERW5M/f7cacTSNrufH19YWDgwPy8vKM5ufl5SEwMLDedYKCguDo6AgHBwdpXteuXaHRaFBVVQUnJyej5VUqFVQqlTT958bhHhwiIiLrU1JSArVa3eAyspYbJycn9OnTBwkJCRgzZgyA2j0zCQkJmD59er3rDBw4EKtXr4bBYIBCUXtULT09HUFBQXWKTX2Cg4ORnZ0Nd3d3CIJgsu8C1Ban0NBQZGdn82TlFsTtbB7czubDbW0e3M7m0VLbWRRFlJSUIDg4+JbLyn5Yavbs2Zg0aRL69u2L6OhoLF68GGVlZZgyZQoAYOLEiQgJCUF8fDwA4LnnnsOSJUswc+ZMzJgxA+fOncMHH3yAF154oVGfp1Ao0Lp16xb7PgDg4eHBvzhmwO1sHtzO5sNtbR7czubREtv5Vnts/iR7uXnsscdw9epVzJ8/HxqNBr169cLWrVulk4yzsrKkPTRA7eGkbdu24cUXX0TPnj0REhKCmTNn4rXXXpPrKxAREZEFkf0+N7aE99AxD25n8+B2Nh9ua/PgdjYPS9jOFjH8gq1QqVR46623jE5gJtPjdjYPbmfz4bY2D25n87CE7cw9N0RERGRTuOeGiIiIbArLDREREdkUlhsiIiKyKSw3REREZFNYbkxk6dKlCA8Ph7OzM2JiYnD48GG5I1mV+Ph49OvXD+7u7vD398eYMWOQlpZmtExlZSWmTZsGHx8ftGrVCg899FCdoTuysrIwatQouLq6wt/fH6+88gpqamrM+VWsysKFCyEIAmbNmiXN43Y2nZycHPztb3+Dj48PXFxc0KNHDyQlJUmvi6KI+fPnIygoCC4uLoiLi8O5c+eM3qOwsBDjx4+Hh4cHPD098dRTT6G0tNTcX8Vi6fV6zJs3D23btoWLiwvat2+P9957z2j8IW7nptu7dy9Gjx6N4OBgCIKATZs2Gb1uqm168uRJDB48GM7OzggNDcWHH35omi8g0m1bu3at6OTkJK5YsUI8deqU+Mwzz4ienp5iXl6e3NGsxvDhw8WvvvpKTE1NFZOTk8V7771XDAsLE0tLS6Vlpk6dKoaGhooJCQliUlKS2L9/f3HAgAHS6zU1NWJERIQYFxcnHj9+XNyyZYvo6+srzpkzR46vZPEOHz4shoeHiz179hRnzpwpzed2No3CwkKxTZs24uTJk8VDhw6JGRkZ4rZt28Tz589LyyxcuFBUq9Xipk2bxBMnToj333+/2LZtW7GiokJaZsSIEWJkZKR48OBB8bfffhM7dOggPvHEE3J8JYu0YMEC0cfHR9y8ebN44cIFcd26dWKrVq3Ef/3rX9Iy3M5Nt2XLFnHu3Lnihg0bRADixo0bjV43xTYtLi4WAwICxPHjx4upqanimjVrRBcXF/H//u//bjs/y40JREdHi9OmTZOm9Xq9GBwcLMbHx8uYyrpduXJFBCDu2bNHFEVRLCoqEh0dHcV169ZJy5w5c0YEICYmJoqiWPuXUaFQiBqNRlpm2bJlooeHh6jT6cz7BSxcSUmJ2LFjR3H79u3inXfeKZUbbmfTee2118RBgwbd9HWDwSAGBgaKH330kTSvqKhIVKlU4po1a0RRFMXTp0+LAMQjR45Iy/zyyy+iIAhiTk5Oy4W3IqNGjRKffPJJo3ljx44Vx48fL4oit7Mp/LXcmGqbfv7556KXl5fRvxuvvfaa2Llz59vOzMNSt6mqqgpHjx5FXFycNE+hUCAuLg6JiYkyJrNuxcXFAABvb28AwNGjR1FdXW20nbt06YKwsDBpOycmJqJHjx7S0B0AMHz4cGi1Wpw6dcqM6S3ftGnTMGrUKKPtCXA7m9KPP/6Ivn374pFHHoG/vz+ioqLw73//W3r9woUL0Gg0RttarVYjJibGaFt7enqib9++0jJxcXFQKBQ4dOiQ+b6MBRswYAASEhKQnp4OADhx4gT27duHkSNHAuB2bgmm2qaJiYm44447jAa9Hj58ONLS0nDt2rXbyij72FLWLj8/H3q93ugfegAICAjA2bNnZUpl3QwGA2bNmoWBAwciIiICAKDRaODk5ARPT0+jZQMCAqDRaKRl6vtz+PM1qrV27VocO3YMR44cqfMat7PpZGRkYNmyZZg9ezbeeOMNHDlyBC+88AKcnJwwadIkaVvVty1v3Nb+/v5GryuVSnh7e3Nb/+H111+HVqtFly5d4ODgAL1ejwULFmD8+PEAwO3cAky1TTUaDdq2bVvnPf58zcvLq9kZWW7I4kybNg2pqanYt2+f3FFsTnZ2NmbOnInt27fD2dlZ7jg2zWAwoG/fvvjggw8AAFFRUUhNTcXy5csxadIkmdPZjh9++AGrVq3C6tWr0b17dyQnJ2PWrFkIDg7mdrZjPCx1m3x9feHg4FDnapK8vDwEBgbKlMp6TZ8+HZs3b8auXbvQunVraX5gYCCqqqpQVFRktPyN2zkwMLDeP4c/X6Paw05XrlxB7969oVQqoVQqsWfPHnz66adQKpUICAjgdjaRoKAgdOvWzWhe165dkZWVBeD6tmro347AwEBcuXLF6PWamhoUFhZyW//hlVdeweuvv47HH38cPXr0wIQJE/Diiy8iPj4eALdzSzDVNm3Jf0tYbm6Tk5MT+vTpg4SEBGmewWBAQkICYmNjZUxmXURRxPTp07Fx40bs3Lmzzq7KPn36wNHR0Wg7p6WlISsrS9rOsbGxSElJMfoLtX37dnh4eNT5kbFXd999N1JSUpCcnCw9+vbti/Hjx0vPuZ1NY+DAgXVuZ5Ceno42bdoAANq2bYvAwECjba3VanHo0CGjbV1UVISjR49Ky+zcuRMGgwExMTFm+BaWr7y8HAqF8U+Zg4MDDAYDAG7nlmCqbRobG4u9e/eiurpaWmb79u3o3LnzbR2SAsBLwU1h7dq1okqlEleuXCmePn1afPbZZ0VPT0+jq0moYc8995yoVqvF3bt3i7m5udKjvLxcWmbq1KliWFiYuHPnTjEpKUmMjY0VY2Njpdf/vET5nnvuEZOTk8WtW7eKfn5+vET5Fm68WkoUuZ1N5fDhw6JSqRQXLFggnjt3Tly1apXo6uoqfvfdd9IyCxcuFD09PcX//e9/4smTJ8UHHnig3stpo6KixEOHDon79u0TO3bsaNeXKP/VpEmTxJCQEOlS8A0bNoi+vr7iq6++Ki3D7dx0JSUl4vHjx8Xjx4+LAMRFixaJx48fFy9evCiKomm2aVFRkRgQECBOmDBBTE1NFdeuXSu6urryUnBL8tlnn4lhYWGik5OTGB0dLR48eFDuSFYFQL2Pr776SlqmoqJCfP7550UvLy/R1dVVfPDBB8Xc3Fyj98nMzBRHjhwpuri4iL6+vuJLL70kVldXm/nbWJe/lhtuZ9P56aefxIiICFGlUoldunQRv/jiC6PXDQaDOG/ePDEgIEBUqVTi3XffLaalpRktU1BQID7xxBNiq1atRA8PD3HKlCliSUmJOb+GRdNqteLMmTPFsLAw0dnZWWzXrp04d+5co8uLuZ2bbteuXfX+mzxp0iRRFE23TU+cOCEOGjRIVKlUYkhIiLhw4UKT5BdE8YbbOBIRERFZOZ5zQ0RERDaF5YaIiIhsCssNERER2RSWGyIiIrIpLDdERERkU1huiIiIyKaw3BAREZFNYbkhIiIim8JyQ0QEQBAEbNq0Se4YRGQCLDdEJLvJkydDEIQ6jxEjRsgdjYiskFLuAEREADBixAh89dVXRvNUKpVMaYjImnHPDRFZBJVKhcDAQKOHl5cXgNpDRsuWLcPIkSPh4uKCdu3aYf369Ubrp6Sk4K677oKLiwt8fHzw7LPPorS01GiZFStWoHv37lCpVAgKCsL06dONXs/Pz8eDDz4IV1dXdOzYET/++GPLfmkiahEsN0RkFebNm4eHHnoIJ06cwPjx4/H444/jzJkzAICysjIMHz4cXl5eOHLkCNatW4cdO3YYlZdly5Zh2rRpePbZZ5GSkoIff/wRHTp0MPqMd955B48++ihOnjyJe++9F+PHj0dhYaFZvycRmYBJxhYnIroNkyZNEh0cHEQ3Nzejx4IFC0RRFEUA4tSpU43WiYmJEZ977jlRFEXxiy++EL28vMTS0lLp9Z9//llUKBSiRqMRRVEUg4ODxblz5940AwDxzTfflKZLS0tFAOIvv/xisu9JRObBc26IyCIMHToUy5YtM5rn7e0tPY+NjTV6LTY2FsnJyQCAM2fOIDIyEm5ubtLrAwcOhMFgQFpaGgRBwOXLl3H33Xc3mKFnz57Sczc3N3h4eODKlSvN/UpEJBOWGyKyCG5ubnUOE5mKi4tLo5ZzdHQ0mhYEAQaDoSUiEVEL4jk3RGQVDh48WGe6a9euAICuXbvixIkTKCsrk17fv38/FAoFOnfuDHd3d4SHhyMhIcGsmYlIHtxzQ0QWQafTQaPRGM1TKpXw9fUFAKxbtw59+/bFoEGDsGrVKhw+fBj/+c9/AADjx4/HW2+9hUmTJuHtt9/G1atXMWPGDEyYMAEBAQEAgLfffhtTp06Fv78/Ro4ciZKSEuzfvx8zZsww7xclohbHckNEFmHr1q0ICgoymte5c2ecPXsWQO2VTGvXrsXzzz+PoKAgrFmzBt26dQMAuLq6Ytu2bZg5cyb69esHV1dXPPTQQ1i0aJH0XpMmTUJlZSU++eQTvPzyy/D19cXDDz9svi9IRGYjiKIoyh2CiKghgiBg48aNGDNmjNxRiMgK8JwbIiIisiksN0RERGRTeM4NEVk8Hj0noqbgnhsiIiKyKSw3REREZFNYboiIiMimsNwQERGRTWG5ISIiIpvCckNEREQ2heWGiIiIbArLDREREdmU/wc/GRLxhESS8gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "preds.int()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lA8-ArLFFuP",
        "outputId": "d40467a4-5a99-4ff0-95fa-fef752bbde53"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0],\n",
              "        [0],\n",
              "        [1],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [1],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [0],\n",
              "        [1],\n",
              "        [0],\n",
              "        [0]], dtype=torch.int32)"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rD5LS7FiHYtm",
        "outputId": "b90bdfdb-45e7-43bf-be24-a8b9b3cd294a"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    }
  ]
}